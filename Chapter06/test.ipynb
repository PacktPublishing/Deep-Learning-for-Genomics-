{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.environ['THEANO_FLAGS'] = \"device=cuda0,force_device=True,floatX=float32\"\n",
    "# import theano\n",
    "# print(theano.config.device)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Layer, Input, Concatenate, Reshape\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auroc(preds, obs):\n",
    "    fpr, tpr, thresholds  = metrics.roc_curve(obs, preds, drop_intermediate=False)\n",
    "    auroc = metrics.auc(fpr,tpr)\n",
    "    return auroc\n",
    "\n",
    "def get_aupr(preds, obs):\n",
    "    precision, recall, thresholds  = metrics.precision_recall_curve(obs, preds)\n",
    "    aupr = metrics.auc(recall,precision)\n",
    "    return aupr\n",
    "\n",
    "def get_aurocs_and_auprs(tpreds, tobs):\n",
    "    tpreds_df = pd.DataFrame(tpreds)\n",
    "    tobs_df = pd.DataFrame(tobs)\n",
    "    \n",
    "    task_list = []\n",
    "    auroc_list = []\n",
    "    aupr_list = []\n",
    "    for task in tpreds_df:\n",
    "        pred = tpreds_df[task]\n",
    "        obs = tobs_df[task]\n",
    "        auroc=round(get_auroc(pred,obs),5)\n",
    "        aupr = round(get_aupr(pred,obs),5)\n",
    "        task_list.append(task)\n",
    "        auroc_list.append(auroc)\n",
    "        aupr_list.append(aupr)\n",
    "    return auroc_list, aupr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"./deepsea_train/\"\n",
    "\n",
    "testmat = scipy.io.loadmat(data_folder+'test.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 21:03:57.403185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 1000, 4)]         0         \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 975, 320)          33600     \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 75, 320)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 75, 320)           0         \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 75, 640)          1640960   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 75, 640)           0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 48000)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 695)               33360695  \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 695)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 690)               480240    \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 690)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,515,495\n",
      "Trainable params: 35,515,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"./model/tbinet.05-0.06.hdf5\")\n",
    "print('model summary')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate averaged AUROC and AUPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transposed = np.transpose(testmat['testxdata'],axes=(0,2,1))\n",
    "test_sub = test_transposed[:1000,125:815,:]\n",
    "test_sub1 = test_transposed[1000:1001,125:815,:]\n",
    "test_sub1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.16350318e-03, 3.56986769e-03, 4.96459287e-03, 5.98314182e-05,\n",
       "        5.63691401e-06, 5.87144447e-03, 2.27183546e-03, 1.24734826e-04,\n",
       "        1.50035555e-03, 1.04052406e-02, 3.81200221e-06, 3.19734863e-05,\n",
       "        5.33751352e-03, 4.96992422e-03, 3.68594914e-03, 7.05450680e-03,\n",
       "        6.29185187e-03, 8.51538334e-06, 5.89465536e-03, 1.23998732e-04,\n",
       "        8.86403769e-03, 5.48501778e-03, 3.11387485e-05, 3.32811731e-03,\n",
       "        1.50680216e-02, 4.56176320e-04, 6.20876113e-03, 5.79900155e-03,\n",
       "        8.37785774e-06, 4.77548238e-05, 8.05204082e-03, 4.23389627e-03,\n",
       "        2.64958455e-03, 3.81802884e-03, 3.36103956e-03, 5.16962912e-03,\n",
       "        4.44996491e-04, 8.76849331e-03, 3.87335196e-03, 5.79721155e-03,\n",
       "        2.08634920e-02, 7.19372620e-05, 5.69326617e-03, 5.87811973e-03,\n",
       "        5.08483732e-03, 6.35245524e-05, 5.05362696e-04, 1.00664701e-03,\n",
       "        1.20766775e-03, 1.27548608e-03, 1.25883453e-04, 2.71891389e-04,\n",
       "        2.99344538e-04, 2.10117123e-05, 1.37693627e-04, 4.13144107e-06,\n",
       "        5.20829815e-07, 2.92590926e-06, 5.44092385e-04, 4.45926678e-04,\n",
       "        2.02005845e-03, 1.06865866e-02, 1.29234474e-02, 1.05896470e-06,\n",
       "        5.28404998e-05, 3.00701801e-03, 1.83305296e-03, 1.22525780e-05,\n",
       "        3.24129476e-04, 7.87218960e-05, 4.84692689e-04, 4.53301254e-05,\n",
       "        7.77760986e-04, 8.91059288e-04, 1.18161160e-02, 4.61547682e-03,\n",
       "        4.93631087e-05, 1.42306424e-04, 7.05910427e-03, 1.22425205e-03,\n",
       "        1.70806197e-06, 5.24617848e-04, 3.61596904e-05, 6.55517098e-04,\n",
       "        8.57835585e-06, 5.21377406e-05, 4.33084182e-03, 2.02522357e-03,\n",
       "        3.74390511e-03, 5.56890991e-05, 1.12918671e-04, 8.54691607e-04,\n",
       "        1.56204260e-04, 5.66813338e-04, 2.61599285e-04, 3.81535392e-05,\n",
       "        9.43191815e-04, 2.03929594e-04, 4.43107774e-06, 1.99742262e-05,\n",
       "        3.09442915e-03, 1.69913471e-03, 2.83046684e-04, 8.94829223e-04,\n",
       "        4.12945915e-03, 1.25776157e-02, 4.69308987e-04, 9.74366849e-04,\n",
       "        1.80033711e-03, 7.53312511e-03, 1.07693151e-04, 7.71919440e-05,\n",
       "        8.57913343e-04, 6.73170653e-05, 2.60823267e-03, 1.17748533e-03,\n",
       "        1.12155301e-03, 3.29971407e-03, 1.46943098e-03, 2.53546704e-03,\n",
       "        5.34368301e-05, 3.43768392e-03, 6.39982754e-05, 1.37810200e-03,\n",
       "        1.19978096e-03, 9.94506874e-04, 1.13663008e-03, 7.46885431e-04,\n",
       "        3.98919394e-04, 1.75049168e-03, 1.05342979e-03, 1.92282954e-03,\n",
       "        2.88460404e-04, 7.44911260e-04, 4.87061159e-04, 1.31875393e-04,\n",
       "        6.44618376e-06, 3.43278982e-03, 1.29428052e-03, 6.58302379e-05,\n",
       "        2.89384043e-03, 6.03275148e-05, 2.49418197e-04, 7.68987320e-06,\n",
       "        1.29879263e-04, 7.91114930e-04, 2.92823184e-04, 1.35775050e-03,\n",
       "        7.99264126e-06, 6.15516212e-03, 3.39701583e-05, 9.42369734e-05,\n",
       "        4.75889356e-05, 7.36281218e-04, 4.35362563e-05, 8.35991523e-04,\n",
       "        1.30402841e-04, 2.28107604e-03, 4.61798598e-04, 5.18568384e-04,\n",
       "        7.39083684e-04, 1.19841320e-03, 3.07903695e-03, 7.92585220e-03,\n",
       "        9.78425960e-04, 1.43558398e-04, 1.46553779e-04, 2.57285283e-05,\n",
       "        4.65837028e-03, 7.77121284e-04, 1.17086369e-04, 1.08918618e-06,\n",
       "        5.94501871e-05, 9.36057477e-05, 8.18367116e-03, 3.08101531e-03,\n",
       "        1.80573785e-03, 3.40169470e-04, 7.90438673e-04, 1.28564087e-03,\n",
       "        6.94320071e-04, 9.57281620e-04, 4.38959134e-04, 1.40269403e-03,\n",
       "        2.46434851e-04, 9.26421649e-07, 6.19868515e-04, 2.07335557e-04,\n",
       "        7.54280336e-05, 1.60390482e-05, 1.12053275e-03, 4.39144159e-03,\n",
       "        9.47639998e-03, 3.63067538e-03, 4.45751153e-04, 5.17210877e-03,\n",
       "        3.16222501e-03, 1.29619650e-06, 2.20798043e-04, 5.99989761e-03,\n",
       "        7.29213425e-05, 1.67807471e-03, 1.80334493e-03, 4.44675796e-03,\n",
       "        3.13533492e-05, 2.01604539e-03, 5.35166590e-03, 1.15265569e-03,\n",
       "        6.73062226e-04, 1.33054755e-05, 2.94723548e-04, 4.95721865e-03,\n",
       "        9.78048891e-04, 3.44522134e-03, 1.02858129e-03, 1.27746034e-02,\n",
       "        2.15126332e-02, 8.04419909e-03, 2.47690454e-03, 5.71870827e-04,\n",
       "        3.82722262e-03, 7.36227550e-04, 1.69279010e-04, 2.70843189e-02,\n",
       "        3.17676837e-04, 3.65642458e-03, 9.34264448e-04, 5.02943061e-04,\n",
       "        1.84628437e-03, 7.29378220e-03, 9.94975795e-04, 1.88361341e-03,\n",
       "        1.19566359e-03, 9.76529118e-05, 1.10640901e-03, 2.58017506e-04,\n",
       "        3.86097672e-04, 4.56899783e-04, 3.22711468e-03, 3.19285136e-05,\n",
       "        1.17791491e-03, 9.57468990e-04, 1.85191853e-03, 2.00188858e-03,\n",
       "        4.64592129e-03, 5.41886315e-03, 4.49849234e-04, 2.04723943e-02,\n",
       "        4.08790656e-04, 4.24231589e-03, 2.94551696e-03, 4.73907171e-03,\n",
       "        3.75984266e-04, 1.01540363e-05, 1.05950658e-05, 1.04494335e-03,\n",
       "        1.23634376e-02, 1.39276759e-04, 5.46550618e-05, 9.23382933e-04,\n",
       "        1.25484006e-03, 2.00455496e-03, 2.44285469e-03, 6.25442946e-04,\n",
       "        1.44684990e-03, 6.87297841e-04, 6.44640240e-04, 3.70128173e-03,\n",
       "        1.10391375e-04, 1.64057928e-04, 2.71365658e-04, 4.26030369e-04,\n",
       "        1.68152721e-04, 3.15383717e-04, 4.40353062e-04, 3.73398513e-03,\n",
       "        3.70374619e-05, 1.12903916e-04, 4.57805116e-04, 1.41762648e-04,\n",
       "        1.73069490e-03, 7.87232068e-07, 5.15629246e-04, 7.52474996e-04,\n",
       "        1.48512819e-03, 1.08909408e-05, 9.36797733e-05, 2.77185580e-04,\n",
       "        1.31344621e-03, 2.10574535e-06, 5.43442508e-03, 5.70271077e-05,\n",
       "        5.87410061e-03, 6.53364288e-04, 1.10630797e-04, 7.69693070e-05,\n",
       "        9.44895903e-04, 9.56131425e-03, 2.36830302e-03, 4.77821523e-05,\n",
       "        5.55362727e-04, 7.92810442e-06, 1.63286435e-03, 1.26341271e-04,\n",
       "        6.90843372e-05, 1.30159070e-03, 2.77979462e-03, 1.17015722e-03,\n",
       "        8.88093084e-04, 6.45278124e-05, 1.64717378e-03, 1.61115313e-05,\n",
       "        1.12542138e-03, 1.28276914e-03, 4.78752045e-04, 8.61286662e-06,\n",
       "        8.22849688e-04, 3.00281215e-04, 8.43362650e-05, 1.60215204e-04,\n",
       "        7.61072852e-06, 8.30700515e-07, 1.66487985e-03, 9.08236325e-06,\n",
       "        7.80913797e-06, 2.10354617e-03, 1.30788307e-03, 3.76542041e-04,\n",
       "        4.99736657e-03, 8.23062903e-04, 3.16733075e-03, 1.50152191e-04,\n",
       "        3.51576763e-03, 1.78007940e-05, 4.20849212e-03, 8.09957739e-04,\n",
       "        7.92106520e-03, 2.76973733e-04, 6.29788591e-03, 5.99646228e-05,\n",
       "        5.81066078e-03, 6.86322164e-04, 6.38443526e-05, 8.78325445e-05,\n",
       "        6.81052625e-05, 3.81601043e-04, 2.57956253e-05, 1.34236732e-04,\n",
       "        1.76001235e-03, 1.50905580e-05, 3.96866817e-04, 6.21237414e-05,\n",
       "        9.26782959e-04, 1.83920085e-04, 1.10308043e-04, 1.05908694e-04,\n",
       "        3.06313923e-05, 1.65069196e-03, 1.79942569e-03, 1.26422383e-03,\n",
       "        3.50936753e-04, 2.61519733e-03, 3.57120275e-03, 4.31052363e-03,\n",
       "        6.12118447e-06, 2.88680731e-03, 1.74505834e-03, 3.14418809e-04,\n",
       "        2.84191165e-02, 8.70026555e-03, 7.71435350e-03, 7.68352300e-04,\n",
       "        8.22582115e-06, 5.30698389e-06, 2.38177206e-04, 3.60869672e-05,\n",
       "        9.24892811e-05, 1.51687327e-05, 9.15070064e-04, 3.64151674e-06,\n",
       "        3.43509903e-03, 3.03273380e-04, 1.06117758e-03, 7.40940101e-04,\n",
       "        5.85528232e-06, 3.95667894e-06, 2.48283148e-04, 6.16613688e-05,\n",
       "        6.41052829e-05, 1.64397425e-04, 1.26275397e-03, 3.65663000e-04,\n",
       "        3.08115450e-05, 3.59023485e-04, 7.62038826e-06, 5.64633217e-03,\n",
       "        5.11781406e-03, 1.16684625e-03, 2.16048473e-04, 6.70226931e-04,\n",
       "        1.12586218e-04, 3.61212915e-05, 4.18989873e-03, 9.52936523e-03,\n",
       "        8.16106331e-05, 6.88534928e-04, 1.56765469e-04, 2.09432574e-06,\n",
       "        3.65910376e-03, 8.68972711e-06, 1.16872694e-03, 1.09190287e-05,\n",
       "        8.95572593e-04, 1.95305061e-03, 5.06136394e-06, 2.64452101e-05,\n",
       "        3.56623386e-05, 8.68375122e-04, 2.85361261e-06, 2.54518673e-04,\n",
       "        2.67033647e-06, 1.53037927e-05, 2.11852617e-04, 1.21664652e-03,\n",
       "        2.62983613e-05, 1.36505696e-04, 9.12030868e-04, 2.05176650e-04,\n",
       "        2.66000043e-05, 5.20491274e-04, 8.60967484e-05, 5.76713355e-05,\n",
       "        2.85411777e-04, 2.31570502e-05, 3.00833253e-05, 2.08060359e-04,\n",
       "        1.50708613e-04, 3.58328194e-04, 2.32228922e-04, 7.96410081e-04,\n",
       "        4.05949214e-03, 6.79293927e-03, 3.95774368e-05, 4.37329109e-05,\n",
       "        1.62518347e-07, 4.48156334e-03, 2.04679021e-03, 1.74648230e-04,\n",
       "        2.44181347e-03, 5.74210972e-05, 8.74170742e-04, 6.25258835e-04,\n",
       "        1.02926011e-03, 1.69482664e-04, 9.69236280e-05, 2.88629846e-04,\n",
       "        4.17547972e-06, 5.42569323e-04, 1.55289017e-04, 2.85857527e-06,\n",
       "        7.47288359e-05, 7.76512024e-04, 2.00190768e-03, 4.40854998e-03,\n",
       "        6.77913849e-05, 2.60767154e-02, 4.11626976e-03, 3.53327823e-06,\n",
       "        4.25805221e-04, 6.28084163e-05, 2.78691932e-05, 1.04121327e-05,\n",
       "        5.40705957e-03, 8.60972978e-06, 7.81857943e-06, 1.08676168e-05,\n",
       "        6.84039434e-03, 2.21991795e-03, 6.72321257e-05, 1.20017934e-03,\n",
       "        2.19939975e-04, 6.01327483e-05, 2.78384541e-04, 3.35603181e-05,\n",
       "        1.45019105e-04, 1.76580215e-03, 2.20646895e-03, 1.37363430e-02,\n",
       "        2.82647833e-03, 8.58727936e-03, 3.92788497e-04, 3.05388166e-05,\n",
       "        4.65566246e-03, 6.74897479e-03, 2.20229151e-03, 9.26150568e-03,\n",
       "        1.18900731e-04, 3.74328483e-05, 3.68940848e-04, 2.31814738e-06,\n",
       "        4.22641635e-04, 1.67627614e-02, 1.15215626e-05, 3.79942903e-05,\n",
       "        2.47730728e-04, 1.19467371e-03, 3.32537410e-03, 7.78459385e-03,\n",
       "        1.86935521e-03, 5.26107091e-04, 7.15907736e-05, 1.12453829e-02,\n",
       "        1.79319177e-02, 8.98195663e-04, 1.02062577e-05, 3.90423120e-05,\n",
       "        4.91800456e-05, 6.51405717e-04, 2.74343212e-04, 2.09282036e-03,\n",
       "        2.58749304e-03, 1.55089656e-03, 2.63336534e-03, 1.70986285e-03,\n",
       "        2.11466174e-03, 3.65640910e-04, 1.29652704e-04, 2.81745312e-03,\n",
       "        1.23019290e-05, 1.29039472e-04, 1.29608103e-04, 6.38335041e-06,\n",
       "        1.13991991e-04, 2.80190352e-03, 1.61304354e-06, 2.86821369e-03,\n",
       "        1.48284467e-04, 2.23370153e-04, 4.41620250e-05, 7.30034721e-04,\n",
       "        2.29439334e-04, 2.95591104e-04, 1.96174346e-03, 1.49803964e-04,\n",
       "        5.40290377e-04, 2.60972558e-03, 3.54510878e-04, 2.55505643e-06,\n",
       "        3.45726893e-03, 9.03535075e-03, 1.30945729e-04, 6.04063389e-04,\n",
       "        2.67326552e-03, 2.15029984e-04, 1.72029530e-07, 2.14337910e-04,\n",
       "        8.21291833e-05, 5.04018390e-05, 2.14557920e-04, 4.49281360e-04,\n",
       "        1.01859856e-03, 2.41540271e-04, 1.15796004e-03, 3.59094515e-03,\n",
       "        3.14543815e-03, 2.27377634e-04, 2.53881037e-04, 6.42178056e-05,\n",
       "        3.43673630e-04, 4.00213350e-04, 4.00796183e-04, 7.52705637e-06,\n",
       "        4.22232086e-03, 4.85323049e-04, 1.92423940e-05, 1.71528559e-03,\n",
       "        1.84772490e-03, 1.39022956e-03, 6.95788243e-04, 2.24699223e-04,\n",
       "        1.79017970e-05, 7.78415299e-04, 1.86527432e-05, 1.32955622e-03,\n",
       "        1.28018262e-03, 2.42305570e-03, 3.49103007e-03, 2.91433298e-05,\n",
       "        1.47731102e-04, 4.04095883e-03, 1.35628477e-04, 1.68114420e-04,\n",
       "        1.99426904e-05, 5.15106476e-05, 1.79089478e-03, 6.84992829e-03,\n",
       "        2.56424304e-03, 5.86022483e-03, 4.19967854e-03, 1.58416852e-03,\n",
       "        9.93677648e-04, 3.16984905e-03, 1.50043494e-03, 3.66584514e-03,\n",
       "        2.89519061e-03, 3.64201376e-03, 2.91520939e-03, 2.20108381e-03,\n",
       "        1.17763932e-06, 3.23436875e-03, 1.44392985e-03, 1.34490456e-05,\n",
       "        1.96316885e-03, 4.81217727e-03, 1.04812403e-04, 2.66839564e-03,\n",
       "        4.16889117e-04, 3.02795615e-05, 2.44028680e-03, 6.51844777e-04,\n",
       "        9.17224854e-04, 4.86366078e-03, 6.39316579e-03, 1.65136924e-04,\n",
       "        2.68567121e-03, 9.91424313e-05, 5.68169495e-03, 4.20797849e-03,\n",
       "        4.34993394e-03, 5.20014903e-03, 4.12349217e-03, 5.46654919e-03,\n",
       "        6.37908094e-03, 5.45522943e-03, 6.10640272e-03, 4.36878810e-03,\n",
       "        4.86417813e-03, 2.34355824e-03, 4.82175453e-03, 2.77969544e-03,\n",
       "        1.17624290e-02, 7.36492593e-03, 6.80089323e-03, 5.98534942e-03,\n",
       "        4.34952509e-03, 1.02291815e-02, 6.54942729e-03, 3.51401186e-03,\n",
       "        6.21326175e-03, 4.01709394e-06, 6.65649679e-03, 4.89028124e-03,\n",
       "        5.92676923e-03, 5.56782028e-03, 4.24136361e-03, 4.47480148e-03,\n",
       "        3.67479026e-03, 8.76274146e-03, 5.15554706e-03, 4.25610691e-03,\n",
       "        4.01931955e-03, 9.53527167e-03, 7.29367509e-03, 5.88082569e-03,\n",
       "        5.61484508e-03, 7.29916664e-03, 3.50850145e-03, 6.77909423e-03,\n",
       "        3.85440444e-03, 5.53782471e-03, 1.21376972e-04, 4.74689156e-03,\n",
       "        6.43720897e-03, 6.06520101e-03, 4.52407915e-03, 5.30945184e-03,\n",
       "        4.76427563e-03, 1.61238480e-03, 6.02333294e-03, 7.70767312e-03,\n",
       "        1.60776812e-03, 3.16601526e-03, 6.68029068e-03, 5.96720306e-03,\n",
       "        8.38250574e-03, 8.24750308e-03, 4.61685890e-03, 2.98991799e-03,\n",
       "        4.31108614e-03, 2.54117530e-02]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.transpose(testmat['testxdata'],axes=(0,2,1))[9990:9991,:,:]\n",
    "result = model.predict(x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" \n",
    "    It return softmax values for each sets of scores in x\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(result[:,125:815])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpreds = model.predict(np.transpose(testmat['testxdata'],axes=(0,2,1)),verbose=1)\n",
    "tpreds_temp = np.copy(tpreds)\n",
    "reverse_start_id = int(testmat['testdata'][:,125:815].shape[0]/2)\n",
    "\n",
    "for i in range(reverse_start_id):\n",
    "    tpreds_avg_temp = (tpreds_temp[i] + tpreds_temp[reverse_start_id+i])/2.0\n",
    "    tpreds_temp[i] = tpreds_avg_temp\n",
    "    tpreds_temp[reverse_start_id+i] = tpreds_avg_temp\n",
    "\n",
    "aurocs, auprs = get_aurocs_and_auprs(tpreds_temp,testmat['testdata'][:,125:815])\n",
    "print(\"Averaged AUROC:\",np.nanmean(aurocs))\n",
    "print(\"Averaged AUPR:\", np.nanmean(auprs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('condon_optimzation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c9ad104059c90d12f50de5b72966553aaf7f663dcb89a78d88d441df52afa79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
